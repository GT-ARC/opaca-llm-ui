"""
Request and response models used in the FastAPI routes (and in some of the implementations).
"""
import re
from typing import Iterable, Set, Literal
from typing import List, Dict, Any, Iterator
from datetime import datetime, timezone
import logging
import uuid
import os
import time
import traceback
import asyncio

from litellm.experimental_mcp_client.client import MCPClient
from starlette.websockets import WebSocket
from pydantic import BaseModel, Field, PrivateAttr, SerializeAsAny, ValidationError

from .opaca_client import OpacaClient


logger = logging.getLogger(__name__)


def get_supported_models(supports_structured_output: bool = False):
    def get_env(key: str, alt: str = '') -> list[str]:
        return os.getenv(key, alt).split(";")
    hosts = get_env("LLM_HOSTS", "openai;mistral;anthropic;gemini")
    api_keys = get_env("LLM_API_KEYS", ";;;")
    models = get_env("LLM_MODELS", "gpt-4o-mini,gpt-4o,gpt-5-mini,gpt-5;mistral-medium-latest,magistral-medium-latest;claude-sonnet-4-5,claude-opus-4-1;gemini-2.5-pro,gemini-2.5-flash")
    return [
        (url, key, models.split(","))
        for url, key, models in zip(hosts, api_keys, models)
        if url == "openai" or not supports_structured_output
    ]

# VARIOUS REST REQUEST/RESPONSE CLASSES AND INTERNAL MODEL ELEMENTS

class ConnectRequest(BaseModel):
    """
    Used as payload for the `/connect` route.

    Attributes
        url: The base url to be used for every interaction with the OPACA  platform.
        user: OPACA platform user name (when using auth), or null
        pwd: OPACA platform password (when using auth), or null
    """
    url: str
    user: str | None
    pwd: str | None


class QueryRequest(BaseModel):
    """
    Used as the expected body argument in the `/query/{method}` endpoints

    Attributes
        user_query: The query a user has input into the SAGE ChatBot.
        streaming: whether intermediate results should be streamed via Websocket
    """
    user_query: str
    streaming: bool = False


class AgentMessage(BaseModel):
    """
    Used as the standard response model each time an LLM Agent is invoked.
    It stores individual information generated by the various agents

    Attributes:
        agent: The name of the agent
        content: The content of the message generated by the agent
        tools: List of generated tool calls
        response_metadata: Metadata associated with the response including token usage
        execution_time: Time it took to execute the response
        formatted_output: JSON output generated by the agent, if requested with response_format
    """
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    agent: str
    content: str = ''
    tools: List['ToolCall'] = []
    response_metadata: Dict[str, Any] = {}
    execution_time: float = .0
    formatted_output: Any = None


class QueryResponse(BaseModel):
    """
    The final response that will be sent back to the frontend. Contains a list of `AgentMessages`
    that were generated during the response generation as well as final response or error.

    Attributes:
        query: The original user query that was sent to the backend.
        agent_messages: A list of agent messages that were created during the course of the response generation.
        iterations: The total number of internal iterations that were executed before returning the final response.
        execution_time: The total execution time it took for the selected method to generate an answer.
        content: The generated response that will be shown to the user.
        error: An optional output for any error messages that were generated.
    """
    query: str = ''
    agent_messages: List[AgentMessage] = []
    iterations: int = 0
    execution_time: float = .0
    content: str = ''
    error: str = ''

    @staticmethod
    def from_exception(user_query: str, exception: Exception) -> 'QueryResponse':
        """Convert an exception (generic or OpacaException) to a QueryResponse to be
        returned to the Chat-UI."""
        if isinstance(exception, OpacaException):
            logger.error(f'OpacaException: {exception.error_message}\nTraceback: {traceback.format_exc()}')
            return QueryResponse(query=user_query, content=exception.user_message, error=exception.error_message)
        else:
            logger.error(f'Exception: {exception}\nTraceback: {traceback.format_exc()}')
            return QueryResponse(query=user_query, content='Generation failed', error=str(exception))


class OpacaFile(BaseModel):
    """
    Represents a single uploaded PDF file.

    Attributes:
        file_id: ID assigned after upload
        content_type: MIME type of the file
        file_name: The absolute path to the file
        host_ids: IDs assigned by each host the file has been uploaded to
        suspended: Whether the file should be excluded from future requests
    """
    file_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    content_type: str
    file_name: str
    host_ids: Dict[str, str] = Field(default_factory=dict)
    suspended: bool = False


class ChatMessage(BaseModel):
    """
    Model for storing chat history messages. Represents single messages that are generated
    during the invocation of SAGE. Can be stored as a list to be given as messages
    to a model during invocation.
    Corresponds to OpenAI's 'EasyInputMessageParam' (but simpler)

    Attributes:
        role: Role for the message, one of 'system', 'assistant', 'user', 'function', 'tool', or 'developer'.
        content: The content of the message.
    """
    role: str
    content: str | List[Dict[str, Any]]


class ToolCall(BaseModel):
    id: str
    type: Literal["opaca", "mcp"]
    name: str
    args: Dict[str, Any] = {}
    result: Any | None = None

    def without_id(self):
        """representation for tool without ID field, to be passed back to LLM (ID can be confusing)"""
        return {k: v for k, v in self.model_dump().items() if k != "id"}


class ScheduledTask(BaseModel):
    """
    An LLM Task scheduled for later execution, by sending the query to the LLM at a later time

    Attributes:
        method: the AgentMethod originally used to create this task; will be used again for re-creating it
        task_id: ID given to this task, needed for cancelling tasks
        query: the query that will be played back to the LLM on execution
        next_time: the next time this task should be executed
        interval: interval between executions (or just until the first execution if no repetitions)
        repetitions: how many more times this task should be executed; -1 for infinite (should never be zero)
    """
    method: str
    task_id: int
    query: str
    next_time: str
    interval: int
    repetitions: int


class Chat(BaseModel):
    """
    Stores information about each chat.

    Attributes:
        chat_id: The unique ID of the chat.
        name: human-readable name of the chat (generated or assigned)
        responses: list of full query-responses incl. intermediate messages and meta-infos
        time_created: when the chat was created
        time_modified: when the chat was last used
        messages: Chat history (user queries and final LLM responses), used in subsequent requests. (derived)
    """
    chat_id: str
    name: str = ''
    responses: List[QueryResponse] = []
    time_created: datetime = Field(default_factory=lambda: datetime.now(tz=timezone.utc))
    time_modified: datetime = Field(default_factory=lambda: datetime.now(tz=timezone.utc))

    @property
    def messages(self) -> Iterator[ChatMessage]:
        for r in self.responses:
            yield ChatMessage(role="user", content=r.query)
            yield ChatMessage(role="assistant", content=r.content)

    def store_interaction(self, result: QueryResponse):
        self.responses.append(result)
        self.update_modified()
        self.derive_name()

    def update_modified(self) -> None:
        self.time_modified = datetime.now(tz=timezone.utc)

    def derive_name(self) -> None:
        """derive name from first interaction, if any, and if not set yet"""
        if not self.name and self.responses:
            message = self.responses[0].query or self.responses[0].content
            self.name = (f'{message[:32]}â€¦' if len(message) > 32 else message)


class SessionData(BaseModel):
    """
    Stores relevant information regarding the session, including messages, configuration,
    client instances, API keys, and uploaded files.

    Attributes:
        session_id: The session's internal ID.
        bookmarks: All prompts bookmarked during the session.
        chats: All the chat histories associated with the session.
        config: Configuration dictionary, one sub-dict for each method.
        abort_sent: Boolean indicating whether the current interaction should be aborted.
        uploaded_files: Dictionary storing each uploaded PDF file.
        scheduled_tasks: LLM queries scheduled for later execution by Internal Tools.
        notifications_chats_map: Which notifications should be auto-appended to which chats.
        valid_until: Timestamp until session is active.
        mcp_servers: All added mcp server information in JSON format.
    Transient fields:
        _websocket: Can be used to send intermediate result and other messages back to the UI
        _ws_message_queue: Used to buffer messages received from the websocket
        _ws_out_cache: Used to cache outgoing WS messages if WS is disconnected, to be sent later
        _opaca_client: Client instance for OPACA, for calling agent actions.
        _llm_clients: Dictionary of LLM client instances.
        _user_api_keys: User-provided API keys for specific LLM hosts

    Note: The websocket from the session should not be used directly; instead use the send/receive
    methods. Especially the latter is necessary to ensure that messages are properly received while
    the server is using the same method for waiting for the webserver to be closed again.
    """
    session_id: str = Field(default_factory=lambda: str(uuid.uuid4()), alias='_id')
    bookmarks: list[dict] = Field(default_factory=list)
    chats: Dict[str, Chat] = Field(default_factory=dict)
    config: Dict[str, Any] = Field(default_factory=dict)
    abort_sent: bool = False
    uploaded_files: Dict[str, OpacaFile] = Field(default_factory=dict)
    scheduled_tasks: Dict[int, ScheduledTask] = Field(default_factory=dict)
    notifications_chats_map: Dict[int, Set[str]] = Field(default_factory=dict)
    valid_until: float = -1
    mcp_servers: List[Dict] = Field(default_factory=list)

    _websocket: WebSocket | None = PrivateAttr(default=None)
    _ws_msg_queue: asyncio.Queue | None = PrivateAttr(default=None)
    _ws_out_cache: list[dict] | None = PrivateAttr(default_factory=list)
    _opaca_client: OpacaClient = PrivateAttr(default_factory=OpacaClient)
    _user_api_keys: Dict[str, str] = PrivateAttr(default_factory=dict)

    @property
    def opaca_client(self) -> OpacaClient:
        return self._opaca_client

    def is_valid(self) -> bool:
        return self.valid_until > time.time()

    def get_config(self, method) -> 'MethodConfig':
        config = self.config.get(method.NAME, method.CONFIG())
        if isinstance(config, dict):  # config is deserialized from DB as dict since the exact type is not known then
            try:
                config = self.config[method.NAME] = method.CONFIG(**config)
            except ValidationError as e:
                logger.warning(f"Config does not comply with schema; resetting to default.")
                config = self.config[method.NAME] = method.CONFIG()
        return config

    def get_or_create_chat(self, chat_id: str, create_if_missing: bool = False) -> Chat:
        chat = self.chats.get(chat_id)
        if chat is None and create_if_missing:
            chat = Chat(chat_id=chat_id)
            self.chats[chat_id] = chat
        elif chat is None and not create_if_missing:
            raise KeyError(f"Chat {chat_id} not found")
        return chat

    def delete_chat(self, chat_id: str) -> bool:
        if chat_id in self.chats:
            del self.chats[chat_id]
            return True
        return False

    def set_api_key(self, model: str, api_key: str):
        self._user_api_keys[model.rsplit("/", 1)[0]] = api_key

    def get_api_key(self, model: str) -> str | None:
        return self._user_api_keys.get(model.rsplit("/", 1)[0], None)

    def has_websocket(self) -> bool:
        return self._websocket is not None

    async def websocket_send_pending(self):
        for msg in self._ws_out_cache:
            await self._websocket.send_json(msg)
        self._ws_out_cache.clear()

    async def websocket_send(self, message: BaseModel) -> bool:
        """Send object as JSON over websocket. The JSON will include the class name as "type"."""
        typed_message = {"type": message.__class__.__name__, **message.model_dump()}
        if self._websocket:
            await self._websocket.send_json(typed_message)
            return True
        else:
            self._ws_out_cache.append(typed_message)
            return False

    async def websocket_receive(self) -> dict:
        if self._websocket and self._ws_msg_queue:
            return await self._ws_msg_queue.get()
        else:
            raise Exception("Websocket not connected")

    async def get_mcp_tools(self) -> Dict:
        """Returns a list of all available mcp server tools"""
        tools = {}
        for mcp_server in self.mcp_servers:
            client = MCPClient(server_url=mcp_server["server_url"])
            tools[mcp_server["server_label"]] = await client.list_tools()
        return tools

    def add_mcp_server(self, mcp_server: Dict[str, Any]) -> bool:
        """Adds a new mcp server json"""

        # Check if the server_url field is existing
        if "server_url" not in mcp_server:
            raise OpacaException("The 'server_url' field is required.", "No 'server_url' provided!")

        # Check if the server url is in a valid format:
        if not re.match(r'^https?://', mcp_server["server_url"]):
            raise OpacaException("The 'server_url' needs to be in a valid url-format (e.g. 'http://<address>.com/mcp')", "Malformed 'server_url'!")

        # Check if a previous mcp server with the same url already exists
        if any(m["server_url"] == mcp_server["server_url"] for m in self.mcp_servers):
            raise OpacaException(f"An MCP server with the given server_url '{mcp_server['server_url']}' already exists!", "Duplicate 'server_url'!")

        # If no server label was given, transform the server_url into the label
        if not mcp_server["server_label"]:
            mcp_server["server_label"] = re.sub(r'^.*//([^/]+).*$', r'\1', mcp_server["server_url"]).replace('.', '-')

        self.mcp_servers.append(mcp_server)
        return True

    def delete_mcp_server(self, mcp_name: str) -> bool:
        """Deletes an mcp server based on its name. The UI only stores the server_label."""
        for mcp in self.mcp_servers:
            if (mcp["server_label"]) == mcp_name:
                self.mcp_servers.remove(mcp)
                return True
        return False

    def prune_notifications_chats_map(self):
        """Remove orphaned taskIds and invalid chatIds."""
        to_delete = []

        for task_id, chat_ids in list(self.notifications_chats_map.items()):
            # Delete map entries for tasks no longer scheduled
            if task_id not in self.scheduled_tasks:
                to_delete.append(task_id)
                continue

            # Remove chat_ids that no longer exist
            if cleaned := {cid for cid in chat_ids if cid in self.chats}:
                self.notifications_chats_map[task_id] = cleaned
            else:
                to_delete.append(task_id)

        # Remove any empty / invalid entries
        for task_id in to_delete:
            del self.notifications_chats_map[task_id]


class SearchResult(BaseModel):
    """
    Result to some search query, showing in which chat and message the string was found.

    Attributes:
        chat_id: id of the chat where the string was found
        chat_name: name of the chat where the string was found
        message_id: id of the message  where the string was found
        excerpt: some "context" showing where the string was found
    """
    chat_id: str
    chat_name: str
    message_id: int
    excerpt: str


# CUSTOM EXCEPTIONS

class OpacaException(Exception):
    """
    Custom Exception class that allows to return both a user-message (shown directly in the chat bubble)
    and error message (shown when clicking on the error-marker), as well as a status code.
    """

    def __init__(self, user_message: str, error_message: str = "", status_code: int = 400):
        super().__init__(user_message)
        self.user_message = user_message
        self.error_message = error_message
        self.status_code = status_code


# MCP MESSAGES

class MCPCreateMessage(BaseModel):
    content: Dict[str, Any]


class MCPDeleteMessage(BaseModel):
    name: str


# MESSAGES SENT OR RECEIVED VIA WEBSOCKET

class TextChunkMessage(BaseModel):
    id: str
    agent: str
    chunk: str
    is_output: bool


class ToolCallMessage(BaseModel):
    agent: str
    id: str
    name: str
    args: Dict[str, Any] = {}


class ToolResultMessage(BaseModel):
    id: str
    result: Any | None


class StatusMessage(BaseModel):
    agent: str
    status: str


class MetricsMessage(BaseModel):
    metrics: dict
    execution_time: float


class PushAdvert(BaseModel):
    """Announcement for incoming PushMessage/Notification"""
    task_id: int
    query: str


class PushMessage(QueryResponse):
    """
    Basically just a QueryResponse, but sent via websocket at the end of "execute-later" task.

    Attributes:
        task_id: The scheduled task the PushMessage belongs to
    """
    model_config = {"extra": "ignore"}
    task_id: int


class ContainerLoginNotification(BaseModel):
    """
    This is a helper class to store information regarding the initiated container login.

    Attributes:
        message: An optional message to send to the frontend
        tool_name: The name of the tool that requires further credentials
        retry: Whether the login attempt has already been tried
    """
    container_name: str = ""
    tool_name: str = ""
    retry: bool = False


class ContainerLoginResponse(BaseModel):
    """
    Response to ContainerLoginNotification

    Attributes:
        username: username to use for container login
        password: password to use for container login
        timeout: automatically logout after this many seconds
    """
    username: str
    password: str
    timeout: int


class MissingApiKeyNotification(BaseModel):
    """
    Notify about missing or invalid api keys for a specific model

    Attributes:
        is_invalid: True - The api key is invalid, False - The api key is missing
        model: The model for which the api key is missing or invalid
    """
    is_invalid: bool
    model: str


class MissingApiKeyResponse(BaseModel):
    api_key: str


# METHOD CONFIGURATION

class MethodConfig(BaseModel):
    """
    Base model class that all method config classes should inherit from.
    Provides some static helper functions for easier field definitions.
    """

    @staticmethod
    def string(default: str, options: Iterable[str] = None, allow_free_input: bool = True, title: str = None, description: str = None, regex: str = None) -> Any:
        options = list(options)
        pattern = regex or (None if allow_free_input else re.compile('|'.join(options)))
        return Field(default=default, json_schema_extra={'options': options, 'allow_free_input': allow_free_input}, title=title, description=description, pattern=pattern)

    @staticmethod
    def integer(default: int, min: int, max: int, step: int, title: str = None, description: str = None) -> Any:
        return Field(default=default, ge=min, le=max, multiple_of=step, title=title, description=description)

    @staticmethod
    def number(default: float, min: float, max: float, step: float, title: str = None, description: str = None) -> Any:
        return Field(default=default, title=title, description=description, ge=min, le=max, multiple_of=step)

    @staticmethod
    def boolean(default: bool = False, title: str = None, description: str = None) -> Any:
        return Field(default=default, title=title, description=description,)

    @staticmethod
    def llm_field(title: str = None, description: str = None, supports_structured_output: bool = False) -> Any:
        models = [f"{url}/{model}" for url, _, models in get_supported_models(supports_structured_output) for model in models]
        regex = r"(?P<host>.+)/(?P<model>[\w-]+)" # the named groups are just for a better error message
        return MethodConfig.string(default=models[0], options=models, allow_free_input=True, title=title, description=description, regex=regex)

    @staticmethod
    def temperature_field(default: float = 0, min: float = 0, max: float = 2, step: float = 0.1) -> Any:
        return MethodConfig.number(default=default, min=min, max=max, step=step, title='Temperature', description='Temperature for the models')

    @staticmethod
    def max_rounds_field(default: int = 5, min: int = 1, max: int = 10, step: int = 1) -> Any:
        return MethodConfig.integer(default=default, min=min, max=max, step=step, title='Max Rounds', description='Maximum number of retries')


class ConfigPayload(BaseModel):
    """
    Stores the actual values of a given configuration and the schema that is defined in `ConfigParameter`.

    Attributes
        value: Should be a JSON storing the actual values of parameters in the format `{"key": value}`
        config_schema: A JSON holding the configuration schema definition (same keys as in `value`)
    """
    config_values: SerializeAsAny[MethodConfig]
    config_schema: Dict[str, Any]
