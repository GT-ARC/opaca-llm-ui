"""
Request and response models used in the FastAPI routes (and in some of the implementations).
"""
import time
from typing import List, Dict, Any

from langchain_core.language_models import BaseChatModel
from langchain_core.messages import AIMessage
from langchain_openai import ChatOpenAI
from pydantic import BaseModel

from Backend.src.utils import build_prompt


class Url(BaseModel):
    url: str
    user: str | None
    pwd: str | None


class Message(BaseModel):
    user_query: str
    debug: bool
    api_key: str


class AgentMessage(BaseModel):
    """
    Stores individual information generated by the various agents
    """
    agent: str
    content: str = ''
    tools: List[str] = []
    response_metadata: Dict[str, Any] = {}
    execution_time: float = .0


class Response(BaseModel):
    """
    Stores relevant information that have been generated by the backends
    """
    query: str = ''
    agent_messages: List[AgentMessage] = []
    iterations: int = 0
    execution_time: float = .0
    content: str = ''
    error: str = ''


class SessionData(BaseModel):
    """
    Stores relevant information regarding the session
    """
    messages: List[Any] = []
    config: Dict[str, Any] = {}
    client: Any = None
    api_key: str = None


class OpacaLLMBackend:
    NAME: str
    llm: BaseChatModel | ChatOpenAI     # TODO maybe extend the types to support different future models

    async def query(self, message: str, session: SessionData) -> Response:
        pass

    async def defaultConfig(self) -> Dict[str, Any]:
        pass


class LLMAgent:
    name: str
    llm: BaseChatModel | ChatOpenAI
    system_prompt: str
    examples: List = []
    input_variables: List[str] = []
    message_template: str = ''
    tools: List = []

    def invoke(self, **kwargs):
        exec_time = time.time()
        prompt = build_prompt(
            system_prompt=self.system_prompt,
            examples=self.examples,
            input_variables=self.input_variables,
            message_template=self.message_template,
        )

        chain = prompt | self.llm.bind_tools(tools=self.tools)

        result = chain.invoke(**kwargs)

        # Checks if answer was generated by integrated model class or Llama proxy
        res_meta_data = {}
        if isinstance(result, AIMessage):
            res_meta_data = result.response_metadata.get("token_usage", {})
            result = result.content

        return AgentMessage(
            name=self.name,
            content=result,
            tools=result.get('tool_calls', []),
            response_metadata=res_meta_data,
            execution_time=time.time() - exec_time,
        )
