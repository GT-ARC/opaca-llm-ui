# Models

This file explains the different standardized model classes that are used throughout the OPACA LLM. Most of these classes are located in the file [models.py](../Backend/src/models.py).

### AgentMessage

#### Description

The Agent Message is used as the standard response model each time an LLM Agent is invoked. It holds different information pertaining to the LLM invocation.

#### Attributes
| Attribute           | Type             | Default | Description                                                       |
|---------------------|------------------|---------|-------------------------------------------------------------------|
| `agent`             | `str`            |         | The name of the agent.                                            |
| `content`           | `str`            |         | The text output this agent has generated.                         |
| `tools`             | `List[Any]`      | `[]`    | A list of optional tools that were generated by this agent.       |
| `response_metadata` | `Dict[str, Any]` | `{}`    | A object containing additional information regarding token usage. |
| `execution_time`    | `float`          | `.0`    | The total time it took for this agent to generate its output.     |

### Response

#### Description

The final response that will be sent back to the frontend. Contains a list of `AgentMessages` that were generated during the response generation.

#### Attributes
| Attribute        | Type                 | Default | Description                                                                                     |
|------------------|----------------------|---------|-------------------------------------------------------------------------------------------------|
| `query`          | `str`                | `""`    | The original user query that was sent to the backend.                                           |
| `agent_messages` | `List[AgentMessage]` | `[]`    | A list of agent messages that were created during the course of the response generation.        |
| `iterations`     | `int`                | `0`     | The total number of internal iterations that were executed before returning the final response. |
| `execution_time` | `float`              | `.0`    | The total execution time it took for the selected method to generate an answer.                 |
| `content`        | `str`                | `""`    | The generated response that will be shown to the user.                                          |
| `error`          | `str`                | `""`    | An optional output for any error messages that were generated.                                  |

### SessionData

#### Description

Stores user specific data related to a single browser session.

#### Attributes
| Attribute  | Type             | Default | Description                                                                                                                                                                                                                      |
|------------|------------------|---------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `messages` | `List[Any]`      | `[]`    | Saves message pairs consisting of the original user input and the generated output. Is used as a message history in all subsequent requests.                                                                                     |
| `config`   | `Dict[str, Any]` | `{}`    | Stores the configuration setting for each available method. The method names are the keys for the different configuration objects. (Example: `{"tool-llm-llama": {"model": ...}}`                                                |
| `client`   | `OpacaClient`    | `None`  | A personal proxy client. The client is initialized with the given user credentials and authenticated. Every action invocation by a user is subsequently performed by using this client and therefore the associated permissions. |
| `api_key`  | `str`            | `None`  | An optional api key to be used for model access instead of keys stored in the environment variables.                                                                                                                             |

### Message

#### Description

Is used as the expected body argument in the `/{backend}/query` endpoints

#### Attributes
| Attribute    | Type  | Default | Description                                                                  |
|--------------|-------|---------|------------------------------------------------------------------------------|
| `user_query` | `str` |         | The query a user has input into the OPACA LLM ChatBot.                       |
| `api_key`    | `str` | `""`    | An optional API key used instead of keys found in the environment variables. |

### Url

#### Description

Is used as the expected body argument in the `/connect` endpoint.

#### Attributes
| Attribute | Type        | Default | Description                                                                                                                                |
|-----------|-------------|---------|--------------------------------------------------------------------------------------------------------------------------------------------|
| `url`     | `str`       |         | The base url to be used for every interaction with the OPACA platform.                                                                     |
| `user`    | `str\|None` |         | The user name to authenticate against the connected OPACA platform. Only relevant if the connected OPACA platform requires authentication. |
| `pwd`     | `str\|None` |         | The password used in combination with the given user name. Only relevant if the connected OPACA platform requires authentication.          |

### OPACALLMBackend

#### Description

A super class for every implemented backend. Defines the necessary functions and inherits from `ABC`, the Abstract Base Class, to define abstract methods with a decorator.

#### Attributes
| Attribute | Type            | Default | Description                                                                                        |
|-----------|-----------------|---------|----------------------------------------------------------------------------------------------------|
| `NAME`    | `str`           |         | A specific name for the individual backend.                                                        |
| `llm`     | `BaseChatModel` |         | The used llm instance. Should be of type `BaseChatModel` to be used within the LangChain framework |

#### Methods

`default_config()`

- Description: Returns the default configuration for the specific backend/method.

`query(message: str, session: SessionData) -> Response`

- Description: Based on the given user message and the associated session data, generates a llm response.
- Parameters:
  - `message`: The message that should be answered by the selected method
  - `session`: The current session data that was retrieved in the `server.py`
- Returns: A `Response` object including the final response to the user

_async_ `query_stream(message: str, session: SessionData, websocket: starlette.websocket) -> Response`

- Description: Based on the given user message and the associated session data, will stream every new chunk that was generated.
- Parameters:
  - `message`: The message that should be answered by the selected method
  - `session`: The current session data that was retrieved in the `server.py`
  - `websocket`: The websocket used to transmit each new chunk
- Returns: During the message generation `AgentMessage`, as a final object a `Response`

### StreamCallbackHandler

#### Description

This callback handler will be bind to the llm instance and will execute a specific function, every time a new token has been generated. This method is mainly used in combination with `query_stream()`.

#### Attributes
| Attribute       | Type                  | Default | Description                                                                                                                                                                                       |
|-----------------|-----------------------|---------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `agent_message` | `AgentMessage`        |         | A reference to the current agent message for which the llm generation is taking place.                                                                                                            |
| `websocket`     | `starlette.Websocket` |         | The websocket used to transmit each new chunk.                                                                                                                                                    |
| `tool_calls`    | `GenerationChunk`     | `None`  | If tools were generated, this parameter holds the currently constructed tool call.                                                                                                                |
| `first`         | `Boolean`             | `True`  | An indicator to check whether the generated tool call was the first chunk. If it was, `tool_calls` will be set, otherwise an addition will be performed to combine the `GenerationChunk` objects. |

#### Methods

`__init__(agent_message: AgentMessage, websocket: starlette.Websocket)`

- Description: Will initialize the `StreamCallbackhandler`
- Parameters:
  - `agent_message`: A reference to the current agent message for which the llm generation is taking place
  - `websocket`: The websocket used to transmit each new chunk

_async_ `on_llm_new_token(token: str, *, chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None, run_id: UUID, parent_run_id: Optional[UUID] = None, **kwargs: Any) -> Any`

- Description: This method is called every time a new chunk has been generated by the LLM. Will immediately transmit the generated chunk via the given websocket. While each normal text output is sent per token, the tool calls instead are always sent as a whole, each time with new characters added to the formatted JSON object.
- Parameters:
  - `token`: The extracted string content of the newly generated Chunk
  - `chunk`: The complete chunk that was just generated
  - `run_id`: The individual run id associated with the current LLM run
  - `parent_run_id`: An optional parent run id

### LLMAgent

#### Description

Defines a standardized agent class that can be used throughout OPACA LLM when defining new agents within the LangChain framework. This simplifies the initial setup that would be necessary to create a new method within OPACA LLM.

#### Attributes
| Attribute          | Type            | Default | Description                                                                                                                                                                                       |
|--------------------|-----------------|---------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `name`             | `str`           |         | A reference to the current agent message for which the llm generation is taking place.                                                                                                            |
| `llm`              | `BaseChatModel` |         | The websocket used to transmit each new chunk.                                                                                                                                                    |
| `system_prompt`    | `str`           |         | If tools were generated, this parameter holds the currently constructed tool call.                                                                                                                |
| `examples`         | `List`          | `[]`    | An indicator to check whether the generated tool call was the first chunk. If it was, `tool_calls` will be set, otherwise an addition will be performed to combine the `GenerationChunk` objects. |
| `input_variables`  | `List[str]`     | `[]`    | The websocket used to transmit each new chunk.                                                                                                                                                    |
| `message_template` | `str`           | `""`    | If tools were generated, this parameter holds the currently constructed tool call.                                                                                                                |
| `tools`            | `List`          | `[]`    | An indicator to check whether the generated tool call was the first chunk. If it was, `tool_calls` will be set, otherwise an addition will be performed to combine the `GenerationChunk` objects. |

#### Methods

`__init__(name: str, llm: BaseChatModel, system_prompt: str, **kwargs)`

- Description: Initializes a new instance of the LLMAgent
- Parameters:
  - `name`: The name that will be given to this agent
  - `llm`: An already initialized llm instance that will be used to generate output
  - `system_prompt`: The system prompt that will be used in the LLM

async `ainvoke(inputs: Dict[str, Any], websocket: starlette.Websocket = None) -> AgentMessage`

- Description: Will invoke the llm with the given inputs. Will optionally sent each generated chunk via the provided websocket.
- Parameters:
  - `inputs`: A dictionary which will be given to the llm as an input
  - `websocket`: An optional websocket to stream each generated chunk
- Returns: A single `AgentMessage`
