
# Performance Testing

The different backend methods of the OPACA-LLM have been tested within a designated testing environment. Two different questions sets have been created, to test the answer quality of our implemented methods. The first question set, labeled _simple_, contains questions that would only lead to the invocation of exactly one OPACA action. The question set labeled _complex_ contains questions which would result in numerous OPACA action invocations in one or more internal iterations. To evaluate the answer generated by the OPACA-LLM, each question in both question sets includes an expected answer. The actual response by the OPACA-LLM and the expected answer are then compared by a Judge-LLM using _gpt-4o_. A response can either be "helpful" if the answer fulfills every expectation, or "unhelpful" if information is missing or wrong information is being presented.

To run the tests/benchmark, simply execute the `test.py` script in the `test` directory. The script expects a number of parameters, which are explained with the `-h` parameter. The test script will automatically start an OPACA platform and OPACA LLM instance (backend only) using docker-compose and deploy a number of publicly hosted test containers. The result is then written to JSON file in the `test_runs` directory.

**Note:** The script will try to determine your IP address (in your local network, but _not_ 127.0.0.1), which is needed by the OPACA platform to communicate with its containers. This may not work on all systems. Alternatively, you can provide the URL of the OPACA platform (e.g. using `ifconfig` on your system), including protocol and port, using the `-o` parameter.

For example, a call to the test script my look like this (substitute the backend and model to use, as well as your IP if necessary):

```
python3 test.py -s simple -b simple -m gpt-4o-mini -o http://192.168.178.24:8000
```

Following is an overview of the latest results. It presents the amount of questions in a question set that were deemed "helpful" by the Judge-LLM. The method is given with the model that was used for all agents within that model. The only exception being the _orchestration-method_, which can uses different models for the Orchestrator Agent and Worker Agent.

| Method (model)                                                  | _simple_ (_time_)   | _complex_ (_time_) |
|-----------------------------------------------------------------|---------------------|--------------------|
| Tool-Method (gpt-4o)                                            | **22/24** (158.27s) | 16/23 (418.36s)    |
| Tool-Method (gpt-4o-mini)                                       | **22/24** (141.32s) | 12/23 (431.8s)     |
| Tool-Method (Mistral-Small-Instruct)                            | 18/24 (189.02s)     | 13/23 (375.52s)    |
| Orchestration-Method (Qwen25_32B_INT4 & Mistral-Small-Instruct) | **22/24** (202.23s) | 16/23 (764.45s)    |
| Simple (gpt-4o-mini)                                            | 16/24 (n/a)         | **18/23** (n/a)    |

Please keep in mind, these results are only preliminary and the quality of each question has been measured by another LLM (gpt-4o). Therefore, the performance overview only provides a rough estimate of the actual performance of each method.
